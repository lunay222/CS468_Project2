# AI Context-Aware Study Coach

A complete microservice-based mobile application that uses the phone camera (hardware input) to scan notes and provides AI-generated **multiple choice quizzes**, summaries, and flashcards. Features hardware outputs including display, TTS audio, and vibration alerts.

## What It Does

The AI Study Coach app allows you to:
1. **Scan your notes** using your phone's camera (handwritten or printed text)
2. **Extract text** from images using OCR (Optical Character Recognition)
3. **Generate study materials** using AI:
   - **Multiple choice quizzes only** - Interactive questions with immediate feedback (this is the only quiz type generated by the app)
   - **Summaries** - AI-generated summaries of your notes
   - **Flashcards** - Front/back card format for memorization
4. **Study interactively** with:
   - Immediate feedback on quiz answers (correct âœ“ / incorrect âœ—)
   - Text-to-speech to hear questions and summaries
   - Haptic vibration feedback for interactions

## Features

- ğŸ“· **Camera Input**: Scan handwritten or printed notes using phone camera (hardware input)
- ğŸ¤– **AI-Powered Multiple Choice Quiz Generation**: 
  - **Multiple choice questions only** - The mobile app generates and displays only multiple choice quizzes with immediate feedback (correct/incorrect indicators)
  - Interactive quiz interface with tap-to-reveal answers and explanations
  - Note: Other quiz types (fill-in-the-blank, short answer) are supported by the API but not implemented in the mobile app UI
  - Flashcards with front/back card format
  - AI-generated summaries of scanned content
- ğŸ“± **Hardware Outputs**:
  - Display: Multiple choice quiz questions with immediate correct/incorrect feedback and tap-to-reveal answers
  - TTS Audio: Read questions and summaries aloud
  - Vibration: Haptic feedback for interactions (success patterns for correct answers, error patterns for incorrect)
- ğŸ—ï¸ **Microservice Architecture**: 
  - OCR Service (standalone)
  - API Gateway/Orchestrator
  - Ollama LLM Service
- ğŸ³ **Dockerized**: Complete Docker Compose setup
- ğŸ§  **Ollama Integration**: Local LLM for privacy
- âœ… **50%+ AI-Generated Code**: Integration code, API interfaces, prompts

## Project Structure

```
studyCoach/
â”œâ”€â”€ ocr-service/            # OCR Microservice (FastAPI)
â”‚   â”œâ”€â”€ main.py            # OCR service application
â”‚   â”œâ”€â”€ requirements.txt   # Python dependencies
â”‚   â””â”€â”€ Dockerfile         # OCR container config
â”œâ”€â”€ api-gateway/           # API Gateway/Orchestrator (FastAPI)
â”‚   â”œâ”€â”€ main.py           # Gateway application
â”‚   â”œâ”€â”€ requirements.txt  # Python dependencies
â”‚   â””â”€â”€ Dockerfile        # Gateway container config
â”œâ”€â”€ mobile-app/            # React Native mobile app
â”‚   â”œâ”€â”€ App.js            # Main app component
â”‚   â”œâ”€â”€ services/         # API service layer
â”‚   â”‚   â””â”€â”€ api.js        # API Gateway client
â”‚   â””â”€â”€ package.json      # Node dependencies
â”œâ”€â”€ tests/                 # Comprehensive test suite
â”‚   â”œâ”€â”€ test_ocr_service.py    # OCR unit tests
â”‚   â”œâ”€â”€ test_api_gateway.py    # Gateway unit tests
â”‚   â”œâ”€â”€ test_integration.py    # Integration tests
â”‚   â”œâ”€â”€ TESTING_WALKTHROUGH.md  # Testing guide
â”‚   â””â”€â”€ requirements.txt       # Test dependencies
â”œâ”€â”€ docker-compose.yml     # Docker orchestration (all services)
â”œâ”€â”€ DESIGN.md             # Complete architecture design
â”œâ”€â”€ AI_CODE_DOCUMENTATION.md # AI-generated code documentation
â”œâ”€â”€ SETUP_AND_RUN.md      # Setup and run instructions
â””â”€â”€ README.md             # This file
```

## Prerequisites

- Docker and Docker Compose
- Python 3.11+ (for local development)
- Node.js and npm (for mobile app development)
- Expo CLI (for React Native development)

## Quick Start

### 1. Clone the Repository

```bash
git clone <repository-url>
cd studyCoach
```

### 2. Start Docker Services

```bash
# Start all microservices (OCR, API Gateway, Ollama)
docker-compose up -d

# Pull the Ollama model (first time only, may take several minutes)
docker exec study-coach-ollama ollama pull llama3.2:1b

# Verify model is available
docker exec study-coach-ollama ollama list
```

### 3. Verify Services

```bash
# Check API Gateway health
curl http://localhost:8000/health

# Check OCR Service health
curl http://localhost:8001/health

# Check Ollama
curl http://localhost:11434/api/tags
```

### 4. Run Tests

```bash
# Install test dependencies
pip install -r tests/requirements.txt
pip install -r backend/requirements.txt

# Run tests
pytest tests/test_backend.py -v
```

### 5. Start Mobile App

```bash
cd mobile-app
npm install
npm start
```

## API Endpoints

### API Gateway Endpoints

**Health Check**
```
GET /health
```

**Scan Image (OCR)**
```
POST /api/scan
Content-Type: multipart/form-data
Body: file (image file)
Response: {"success": true, "text": "extracted text"}
```

**Generate Quiz (Multiple Choice)**
```
POST /api/generate_quiz
Content-Type: application/json
Body: {
  "text": "extracted text",
  "quiz_type": "multiple_choice" | "all" | "fill_blank" | "short_answer"
}
Response: {
  "success": true,
  "quiz": {
    "multiple_choice": [
      {
        "question": "...",
        "options": ["...", "...", "...", "..."],
        "correct_answer": 0,
        "explanation": "..."
      }
    ]
  }
}
```
Note: The mobile app **only generates multiple choice quizzes**. While the API supports other quiz types (`fill_blank`, `short_answer`, `all`), the mobile app interface is designed exclusively for multiple choice questions with immediate feedback.

**Generate Summary**
```
POST /api/summary
Content-Type: application/json
Body: {"text": "extracted text"}
Response: {"success": true, "summary": "..."}
```

**Generate Flashcards**
```
POST /api/generate_flashcards
Content-Type: application/json
Body: {"text": "extracted text"}
Response: {"success": true, "flashcards": [...]}
```

### OCR Service Endpoints

**Health Check**
```
GET /health
```

**Extract Text**
```
POST /extract
Content-Type: multipart/form-data
Body: file (image file)
Response: {"success": true, "text": "extracted text"}
```

## Configuration

### Environment Variables

**API Gateway** (in `docker-compose.yml`):
- `OCR_SERVICE_URL`: OCR service URL (default: `http://ocr-service:8001`)
- `LLM_SERVICE_URL`: Ollama service URL (default: `http://ollama:11434`)
- `OLLAMA_MODEL`: LLM model name (default: `llama3.2:1b`)
- `TTS_ENABLED`: Enable TTS features (default: `true`)

**OCR Service** (in `docker-compose.yml`):
- `TESSERACT_CMD`: Tesseract executable path (default: `/usr/bin/tesseract`)

**Mobile App** (in `mobile-app/services/api.js`):
- Update `DEFAULT_API_BASE_URL` to your computer's IP address (e.g., `http://192.168.1.100:8000`)

## Development

### Backend Development

```bash
# Install dependencies
cd backend
pip install -r requirements.txt

# Run locally (requires Ollama running)
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Mobile App Development

```bash
cd mobile-app
npm install
npm start

# For iOS
npm run ios

# For Android
npm run android
```

## Testing

### Run All Tests
```bash
# Install test dependencies
pip install -r tests/requirements.txt

# Run all tests
pytest tests/ -v
```

### Run Specific Test Suites
```bash
# OCR Service tests
pytest tests/test_ocr_service.py -v

# API Gateway tests
pytest tests/test_api_gateway.py -v

# Integration tests
pytest tests/test_integration.py -v
```

### Run with Coverage
```bash
pytest tests/ --cov=api-gateway --cov=ocr-service --cov-report=html
```

See `tests/TESTING_WALKTHROUGH.md` for comprehensive testing documentation.

## Troubleshooting

### Ollama Service Not Available
```bash
# Check if Ollama container is running
docker ps | grep ollama

# Restart Ollama
docker-compose restart ollama

# Pull model if missing
docker exec study-coach-ollama ollama pull llama3.2:1b
```

### OCR Not Working
- Ensure Tesseract is installed in the Docker container
- Check image quality and format (JPEG, PNG supported)
- Verify file upload is successful
- Make sure images are clear and well-lit for better text extraction

## Documentation

- **DESIGN.md**: Complete architecture design with microservice diagrams, data flow, and AI tool mapping
- **AI_CODE_DOCUMENTATION.md**: Detailed documentation of AI-generated code (50%+ requirement)
- **SETUP_AND_RUN.md**: Step-by-step setup and run instructions
- **tests/TESTING_WALKTHROUGH.md**: Comprehensive testing guide with debugging walkthroughs
- **TESTING_REPORT.md**: Testing documentation with results
- **REFLECTION.md**: Development reflection on AI-assisted development experience

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request


## Acknowledgments

- Ollama for providing local LLM capabilities
- Tesseract OCR for text extraction
- FastAPI for the web framework
- React Native and Expo for mobile development


